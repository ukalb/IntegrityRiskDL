{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model: LSTM\n",
    "#Word Embedding: Custom\n",
    "#Dataset: 3\n",
    "#based on: http://www.insightsbot.com/blog/1wAqZg/keras-lstm-example-sequence-binary-classification\n",
    "#https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "#https://github.com/keras-team/keras/blob/master/examples/imdb_bidirectional_lstm.py\n",
    "\n",
    "import os\n",
    "\n",
    "def GetTextFilePathsInDirectory(directory):\n",
    "    files = []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".txt\"):\n",
    "            filePath = os.path.join(directory, file)\n",
    "            files.append(filePath)\n",
    "    return files\n",
    "\n",
    "def GetLinesFromTextFile(filePath):\n",
    "    with open(filePath,\"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_files = GetTextFilePathsInDirectory(\"./dataset3/positive\")\n",
    "negative_files = GetTextFilePathsInDirectory(\"./dataset3/negative\")\n",
    "\n",
    "reviews_positive = []\n",
    "for i in range(0,1620):\n",
    "    reviews_positive.extend(GetLinesFromTextFile(positive_files[i]))\n",
    "    \n",
    "reviews_negative = []\n",
    "for i in range(0,1620):\n",
    "    reviews_negative.extend(GetLinesFromTextFile(negative_files[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])$&%=#•~@£©®→°€™›♥←×§″′Â█½à…“★”–●â►¢²¬░¶↑±¿▾¦║―¥▓▒¼⊕▼▪†■▀¨▄♫☆é♦¤▲è¾Ã∞∙↓、,│»♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    default_stop_words = nltk.corpus.stopwords.words('german')\n",
    "    stopwords = set(default_stop_words)\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    reviews = [RemoveStopWords(line,stopwords) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "def RemoveStopWords(line, stopwords):\n",
    "    words = []\n",
    "    for word in line.split(\" \"):\n",
    "        word = word.strip()\n",
    "        if word not in stopwords and word != \"\" and word != \"&\":\n",
    "            words.append(word)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_positive = preprocess_reviews(reviews_positive)\n",
    "reviews_negative = preprocess_reviews(reviews_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Reviews_Labeled = list(zip(reviews_positive, np.ones(len(reviews_positive))))\n",
    "Reviews_Labeled.extend(list(zip(reviews_negative, np.zeros(len(reviews_negative)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self, vocabulary, wordFrequencyFilePath):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.WORD_FREQUENCY_FILE_FULL_PATH = wordFrequencyFilePath\n",
    "        self.input_word_index = {}\n",
    "        self.reverse_input_word_index = {}\n",
    "        \n",
    "        self.MaxSentenceLength = None\n",
    "        \n",
    "    def PrepareVocabulary(self,reviews):\n",
    "        self._prepare_Word_Frequency_Count_File(reviews)\n",
    "        self._create_Vocab_Indexes()\n",
    "        \n",
    "        self.MaxSentenceLength = max([len(txt.split(\" \")) for txt in reviews])\n",
    "      \n",
    "    def Get_Top_Words(self, number_words = None):\n",
    "        if number_words == None:\n",
    "            number_words = self.vocabulary\n",
    "        \n",
    "        chars = json.loads(open(self.WORD_FREQUENCY_FILE_FULL_PATH).read())\n",
    "        counter = Counter(chars)\n",
    "        most_popular_words = {key for key, _value in counter.most_common(number_words)}\n",
    "        return most_popular_words\n",
    "    \n",
    "    def _prepare_Word_Frequency_Count_File(self,reviews):\n",
    "        counter = Counter()    \n",
    "        for s in reviews:\n",
    "            counter.update(s.split(\" \"))\n",
    "            \n",
    "        with open(self.WORD_FREQUENCY_FILE_FULL_PATH, 'w') as output_file:\n",
    "            output_file.write(json.dumps(counter))\n",
    "                 \n",
    "    def _create_Vocab_Indexes(self):\n",
    "        INPUT_WORDS = self.Get_Top_Words(self.vocabulary)\n",
    "\n",
    "        for i, word in enumerate(INPUT_WORDS):\n",
    "            self.input_word_index[word] = i\n",
    "        \n",
    "        for word, i in self.input_word_index.items():\n",
    "            self.reverse_input_word_index[i] = word\n",
    "       \n",
    "    def TransformSentencesToId(self, sentences):\n",
    "        vectors = []\n",
    "        for r in sentences:\n",
    "            words = r.split(\" \")\n",
    "            vector = np.zeros(len(words))\n",
    "\n",
    "            for t, word in enumerate(words):\n",
    "                if word in self.input_word_index:\n",
    "                    vector[t] = self.input_word_index[word]\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            vectors.append(vector)\n",
    "            \n",
    "        return vectors\n",
    "    \n",
    "    def ReverseTransformSentencesToId(self, sentences):\n",
    "        vectors = []\n",
    "        for r in sentences:\n",
    "            words = r.split(\" \")\n",
    "            vector = np.zeros(len(words))\n",
    "\n",
    "            for t, word in enumerate(words):\n",
    "                if word in self.input_word_index:\n",
    "                    vector[t] = self.input_word_index[word]\n",
    "                else:\n",
    "                    pass\n",
    "                    #vector[t] = 2 #unk\n",
    "            vectors.append(vector)\n",
    "            \n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_WORDS = 500\n",
    "vocab = Vocabulary(TOP_WORDS,\"analysislstmbidirectional.vocab\")\n",
    "\n",
    "reviews_text = [line[0] for line in Reviews_Labeled]\n",
    "vocab.PrepareVocabulary(reviews_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews,labels=zip(*Reviews_Labeled)\n",
    "reviews_int = vocab.TransformSentencesToId(reviews)\n",
    "\n",
    "Reviews_Labeled_Int = list(zip(reviews_int,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(Reviews_Labeled_Int, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = list(zip(*train))\n",
    "X_test, y_test = list(zip(*test))\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence \n",
    "max_review_length = 500 \n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length) \n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58620 train sequences\n",
      "14655 test sequences\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (58620, 100)\n",
      "x_test shape: (14655, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ender/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ender/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,658,945\n",
      "Trainable params: 2,658,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "WARNING:tensorflow:From /home/ender/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 58620 samples, validate on 14655 samples\n",
      "Epoch 1/8\n",
      "58620/58620 [==============================] - 615s 10ms/step - loss: 0.5424 - acc: 0.7508 - val_loss: 0.5260 - val_acc: 0.7611\n",
      "Epoch 2/8\n",
      "58620/58620 [==============================] - 504s 9ms/step - loss: 0.5078 - acc: 0.7743 - val_loss: 0.5148 - val_acc: 0.7687\n",
      "Epoch 3/8\n",
      "58620/58620 [==============================] - 503s 9ms/step - loss: 0.4994 - acc: 0.7773 - val_loss: 0.5106 - val_acc: 0.7677\n",
      "Epoch 4/8\n",
      "58620/58620 [==============================] - 506s 9ms/step - loss: 0.4931 - acc: 0.7823 - val_loss: 0.5071 - val_acc: 0.7702\n",
      "Epoch 5/8\n",
      "58620/58620 [==============================] - 504s 9ms/step - loss: 0.4900 - acc: 0.7845 - val_loss: 0.5073 - val_acc: 0.7724\n",
      "Epoch 6/8\n",
      "58620/58620 [==============================] - 521s 9ms/step - loss: 0.4864 - acc: 0.7860 - val_loss: 0.5105 - val_acc: 0.7679\n",
      "Epoch 7/8\n",
      "58620/58620 [==============================] - 553s 9ms/step - loss: 0.4810 - acc: 0.7885 - val_loss: 0.5073 - val_acc: 0.7725\n",
      "Epoch 8/8\n",
      "58620/58620 [==============================] - 515s 9ms/step - loss: 0.4768 - acc: 0.7909 - val_loss: 0.5162 - val_acc: 0.7717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce41d93d68>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=8,\n",
    "validation_data=[X_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
